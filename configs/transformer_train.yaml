# Transformer PPO Training Configuration - Production Training
# Full-scale training with WandB logging

game:
  mode: junqi_8x3
  board_variant: small

training:
  num_iterations: 10000  # Long training for convergence
  eval_every: 200       # Evaluate every 200 iterations
  eval_episodes: 100    # More evaluation games for better statistics
  save_dir: models/transformer_full  # Directory to save models
  device: auto          # Device: cpu, cuda, or auto
  seed: null

agent:
  type: transformer
  # Model Architecture - Medium size for good performance
  d_model: 128          # Transformer embedding dimension
  nhead: 8              # Number of attention heads
  num_layers: 3         # Number of transformer layers
  dim_feedforward: 512  # FFN dimension (4 * d_model)
  dropout: 0.1          # Dropout rate
  
  # PPO Algorithm Parameters
  lr_start: 0.0001      # Starting learning rate (1e-4)
  lr_end: 0.000005      # Ending learning rate (5e-6)
  gamma: 0.99           # Discount factor
  gae_lambda: 0.95      # GAE lambda for advantage estimation
  clip_coef: 0.2        # PPO clipping coefficient
  vf_coef: 0.5          # Value function loss coefficient
  ent_coef_start: 0.02  # Starting entropy coefficient
  ent_coef_end: 0.001   # Ending entropy coefficient
  
  # Training Loop
  num_steps: 1024       # Steps per iteration (larger rollout buffer)
  minibatch_size: 64    # Minibatch size for PPO updates
  update_epochs: 4      # Number of PPO update epochs per iteration
  max_grad_norm: 0.5    # Maximum gradient norm for clipping

wandb:
  enabled: true
  project: junqi-transformer
  entity: null
  run_name: transformer_production_run
  tags:
    - production
    - junqi_8x3
    - ppo
    - self-play
