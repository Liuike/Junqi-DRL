# RPPO Training Configuration - Standard Board Production Training

game:
  mode: junqi_standard
  board_variant: standard

training:
  # Many more episodes needed for the full board
  num_episodes: 30000 # was 10000
  num_iterations: 30000 # kept in sync for logging
  eval_every: 1000 # eval less often (expensive games)
  eval_episodes: 300 # more games per eval to reduce noise
  save_dir: models/rppo_standard
  device: auto # "auto" â†’ cuda if available on Oscar
  seed: null

agent:
  type: rppo

  # RPPO hyperparameters
  lr: 0.00015 # slightly lower LR for more stable updates
  gamma: 0.995 # longer horizons on the full board
  gae_lambda: 0.95
  clip_eps: 0.2
  k_epochs: 5 # a bit more optimization per batch
  value_coef: 0.5
  entropy_coef: 0.02 # more exploration for huge action space
  max_grad_norm: 0.5

  # Network (bigger model for standard board)
  hidden_size: 512 # was 256

  # Training
  batch_size: 256 # was 64; lower to 128 if you hit CUDA OOM

wandb:
  enabled: true
  project: junqi-rppo
  entity: null
  run_name: rppo_production_run_standard
  tags:
    - production
    - junqi_standard
    - rppo
    - self-play
